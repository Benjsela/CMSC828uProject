{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# from flashtorch.utils import apply_transforms, load_image\n",
    "# from flashtorch.saliency import Backprop\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if on a Mac\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = load_image('adorable_puppy.jpg')\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.title('Original image')\n",
    "# plt.axis('off');\n",
    "\n",
    "# noisy = False\n",
    "\n",
    "# if noisy:\n",
    "#     noisy_img = image + np.random.normal(0, 5, (image.size[1],image.size[0], 3))\n",
    "\n",
    "#     # Prepare inputs to be used later\n",
    "\n",
    "#     img = apply_transforms(noisy_img.astype('uint8'))\n",
    "# else:\n",
    "#     img = apply_transforms(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code copied from flashtorch package and modified so that no images are shown\n",
    "class Backprop:\n",
    "    \"\"\"Provides an interface to perform backpropagation.\n",
    "\n",
    "    This class provids a way to calculate the gradients of a target class\n",
    "    output w.r.t. an input image, by performing a single backprobagation.\n",
    "\n",
    "    The gradients obtained can be used to visualise an image-specific class\n",
    "    saliency map, which can gives some intuition on regions within the input\n",
    "    image that contribute the most (and least) to the corresponding output.\n",
    "\n",
    "    More details on saliency maps: `Deep Inside Convolutional Networks:\n",
    "    Visualising Image Classification Models and Saliency Maps\n",
    "    <https://arxiv.org/pdf/1312.6034.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        model: A neural network model from `torchvision.models\n",
    "            <https://pytorch.org/docs/stable/torchvision/models.html>`_.\n",
    "\n",
    "    \"\"\" # noqa\n",
    "\n",
    "    ####################\n",
    "    # Public interface #\n",
    "    ####################\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.gradients = None\n",
    "        self._register_conv_hook()\n",
    "\n",
    "    def calculate_gradients(self,\n",
    "                            input_,\n",
    "                            target_class=None,\n",
    "                            take_max=False,\n",
    "                            guided=False,\n",
    "                            use_gpu=False):\n",
    "\n",
    "        \"\"\"Calculates gradients of the target_class output w.r.t. an input_.\n",
    "\n",
    "        The gradients is calculated for each colour channel. Then, the maximum\n",
    "        gradients across colour channels is returned.\n",
    "\n",
    "        Args:\n",
    "            input_ (torch.Tensor): With shape :math:`(N, C, H, W)`.\n",
    "            target_class (int, optional, default=None)\n",
    "            take_max (bool, optional, default=False): If True, take the maximum\n",
    "                gradients across colour channels for each pixel.\n",
    "            guided (bool, optional, default=Fakse): If True, perform guided\n",
    "                backpropagation. See `Striving for Simplicity: The All\n",
    "                Convolutional Net <https://arxiv.org/pdf/1412.6806.pdf>`_.\n",
    "            use_gpu (bool, optional, default=False): Use GPU if set to True and\n",
    "                `torch.cuda.is_available()`.\n",
    "\n",
    "        Returns:\n",
    "            gradients (torch.Tensor): With shape :math:`(C, H, W)`.\n",
    "\n",
    "        \"\"\" # noqa\n",
    "\n",
    "        if 'inception' in self.model.__class__.__name__.lower():\n",
    "            if input_.size()[1:] != (3, 299, 299):\n",
    "                raise ValueError('Image must be 299x299 for Inception models.')\n",
    "\n",
    "        if guided:\n",
    "            self.relu_outputs = []\n",
    "            self._register_relu_hooks()\n",
    "\n",
    "        if torch.cuda.is_available() and use_gpu:\n",
    "            self.model = self.model.to('cuda')\n",
    "            input_ = input_.to('cuda')\n",
    "\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        self.gradients = torch.zeros(input_.shape)\n",
    "\n",
    "        # Get a raw prediction value (logit) from the last linear layer\n",
    "\n",
    "        output = self.model(input_)\n",
    "\n",
    "        # Don't set the gradient target if the model is a binary classifier\n",
    "        # i.e. has one class prediction\n",
    "\n",
    "        if len(output.shape) == 1:\n",
    "            target = None\n",
    "        else:\n",
    "            _, top_class = output.topk(1, dim=1)\n",
    "\n",
    "            # Create a 2D tensor with shape (1, num_classes) and\n",
    "            # set all element to zero\n",
    "\n",
    "            target = torch.FloatTensor(1, output.shape[-1]).zero_()\n",
    "\n",
    "            if torch.cuda.is_available() and use_gpu:\n",
    "                target = target.to('cuda')\n",
    "\n",
    "            if (target_class is not None) and (top_class != target_class):\n",
    "                warnings.warn(UserWarning(\n",
    "                    f'The predicted class index {top_class.item()} does not' +\n",
    "                    f'equal the target class index {target_class}. ' +\n",
    "                    'Calculating the gradient w.r.t. the predicted class.'\n",
    "                ))\n",
    "\n",
    "            # Set the element at top class index to be 1\n",
    "\n",
    "            target[0][top_class] = 1\n",
    "\n",
    "        # Calculate gradients of the target class output w.r.t. input_\n",
    "\n",
    "        output.backward(gradient=target)\n",
    "\n",
    "        # Detach the gradients from the graph and move to cpu\n",
    "\n",
    "        gradients = self.gradients.detach().cpu()[0]\n",
    "\n",
    "        if take_max:\n",
    "            # Take the maximum across colour channels\n",
    "\n",
    "            gradients = gradients.max(dim=0, keepdim=True)[0]\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def visualize(self, input_, target_class, guided=False, use_gpu=False,\n",
    "                  figsize=(16, 4), cmap='viridis', alpha=.5,\n",
    "                  return_output=False):\n",
    "        \"\"\"Calculates gradients and visualizes the output.\n",
    "\n",
    "        A method that combines the backprop operation and visualization.\n",
    "\n",
    "        It also returns the gradients, if specified with `return_output=True`.\n",
    "\n",
    "        Args:\n",
    "            input_ (torch.Tensor): With shape :math:`(N, C, H, W)`.\n",
    "            target_class (int, optional, default=None)\n",
    "            take_max (bool, optional, default=False): If True, take the maximum\n",
    "                gradients across colour channels for each pixel.\n",
    "            guided (bool, optional, default=Fakse): If True, perform guided\n",
    "                backpropagation. See `Striving for Simplicity: The All\n",
    "                Convolutional Net <https://arxiv.org/pdf/1412.6806.pdf>`_.\n",
    "            use_gpu (bool, optional, default=False): Use GPU if set to True and\n",
    "                `torch.cuda.is_available()`.\n",
    "            figsize (tuple, optional, default=(16, 4)): The size of the plot.\n",
    "            cmap (str, optional, default='viridis): The color map of the\n",
    "                gradients plots. See avaialable color maps `here <https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html>`_.\n",
    "            alpha (float, optional, default=.5): The alpha value of the max\n",
    "                gradients to be jaxaposed on top of the input image.\n",
    "            return_output (bool, optional, default=False): Returns the\n",
    "                output(s) of optimization if set to True.\n",
    "\n",
    "        Returns:\n",
    "            gradients (torch.Tensor): With shape :math:`(C, H, W)`.\n",
    "        \"\"\" # noqa\n",
    "\n",
    "        # Calculate gradients\n",
    "        gradients = self.calculate_gradients(input_,\n",
    "                                             target_class,\n",
    "                                             guided=guided,\n",
    "                                             use_gpu=use_gpu)\n",
    "        max_gradients = self.calculate_gradients(input_,\n",
    "                                                 target_class,\n",
    "                                                 guided=guided,\n",
    "                                                 take_max=True,\n",
    "                                                 use_gpu=use_gpu)\n",
    "\n",
    "        if return_output:\n",
    "            return gradients, max_gradients\n",
    "\n",
    "    #####################\n",
    "    # Private interface #\n",
    "    #####################\n",
    "\n",
    "    def _register_conv_hook(self):\n",
    "        def _record_gradients(module, grad_in, grad_out):\n",
    "            if self.gradients.shape == grad_in[0].shape:\n",
    "                self.gradients = grad_in[0]\n",
    "\n",
    "        for _, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.modules.conv.Conv2d):\n",
    "                module.register_backward_hook(_record_gradients)\n",
    "                break\n",
    "\n",
    "    def _register_relu_hooks(self):\n",
    "        def _record_output(module, input_, output):\n",
    "            self.relu_outputs.append(output)\n",
    "\n",
    "        def _clip_gradients(module, grad_in, grad_out):\n",
    "            relu_output = self.relu_outputs.pop()\n",
    "            clippled_grad_out = grad_out[0].clamp(0.0)\n",
    "\n",
    "            return (clippled_grad_out.mul(relu_output),)\n",
    "\n",
    "        for _, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.register_forward_hook(_record_output)\n",
    "                module.register_backward_hook(_clip_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_helper(model_module, tensor=img, k=84):\n",
    "#     model = model_module(pretrained=True)\n",
    "#     backprop = Backprop(model)\n",
    "#     backprop.visualize(tensor, k, guided=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_helper(models.alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_helper(models.resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_helper_selftrained(model, tensor, k=0):\n",
    "    tensor.requires_grad = True\n",
    "    backprop = Backprop(model)\n",
    "    gradients, max_gradients = backprop.visualize(tensor, k, alpha = 0, return_output = True)\n",
    "    return gradients, max_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[[-1.1277e-05,  1.8824e-05, -3.8279e-05, -3.8731e-06, -5.2665e-05,\n",
      "           4.1633e-05, -2.4407e-05,  6.2834e-06, -1.1464e-05,  2.6549e-06,\n",
      "          -1.1584e-05, -2.0774e-05, -2.0471e-05, -1.4707e-05,  1.3947e-05,\n",
      "          -8.8340e-06, -3.8955e-05, -2.0453e-05, -5.6794e-05, -9.7095e-07,\n",
      "          -9.8886e-06,  2.5713e-06,  1.5956e-05, -8.3633e-06, -1.5061e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.3657e-05, -1.0110e-05, -4.6235e-05, -8.9595e-06, -6.9583e-05,\n",
      "          -2.5060e-05, -3.0491e-05, -2.8314e-05, -2.0749e-05, -2.7613e-06,\n",
      "           3.9839e-05,  2.5421e-05,  7.5799e-05,  3.4449e-05,  6.2659e-05,\n",
      "           9.3621e-06, -1.9564e-06, -2.6307e-05, -5.6395e-05, -3.1547e-05,\n",
      "          -3.2776e-05, -5.4939e-06, -2.4111e-05, -1.1651e-05, -1.3070e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.5313e-05,  7.8748e-06, -1.5215e-05,  3.0899e-05,  2.2996e-05,\n",
      "          -1.4476e-05,  1.2408e-05,  9.1986e-06, -2.7321e-05,  1.2912e-05,\n",
      "           2.6179e-05,  1.6900e-05,  1.2275e-05,  2.9757e-05,  2.5092e-05,\n",
      "           1.6016e-05, -1.3481e-05,  2.0485e-05,  4.0008e-05,  9.2488e-06,\n",
      "           3.0281e-05,  1.0147e-05,  4.4248e-05, -3.7181e-06,  4.7652e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1021e-05,  1.9443e-05, -8.5388e-06, -2.4220e-05,  3.4260e-06,\n",
      "          -5.5234e-05, -7.4923e-05, -1.0124e-04, -1.0014e-04, -5.1780e-05,\n",
      "          -6.3666e-05, -2.1616e-05, -1.2348e-04, -5.0126e-05, -9.3286e-06,\n",
      "           9.5426e-06, -4.1682e-05, -7.7692e-05, -8.7979e-05, -7.8450e-05,\n",
      "          -7.0241e-05, -6.4717e-05, -3.3099e-05,  6.6027e-06,  9.7307e-06,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.1131e-05,  6.2294e-05,  9.2415e-05,  4.8043e-05,  8.5796e-05,\n",
      "          -7.1267e-05, -1.3349e-04, -5.8058e-05, -1.3490e-04, -8.4827e-05,\n",
      "          -3.4669e-05, -1.6281e-05,  9.4242e-05, -3.6459e-06, -2.8613e-05,\n",
      "          -9.3806e-05, -5.7677e-05, -8.7463e-06,  7.4939e-05, -1.9958e-05,\n",
      "          -2.9794e-05, -3.9526e-05, -5.7152e-07,  1.2541e-05,  8.3494e-06,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.7140e-05, -1.7477e-05,  3.1196e-05, -2.4769e-05, -2.4989e-06,\n",
      "          -9.8117e-05, -1.0291e-05, -1.5992e-05,  1.4240e-04,  2.6062e-05,\n",
      "           3.0272e-04,  1.4833e-04,  1.6896e-04,  5.7777e-05, -3.6158e-05,\n",
      "          -1.0974e-04, -1.9176e-04, -1.5112e-04, -2.2779e-04, -1.6642e-04,\n",
      "          -1.4242e-04, -1.0982e-04, -1.6184e-05,  8.3873e-06,  1.6943e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-9.8062e-06, -1.3469e-05,  1.1386e-04,  8.5823e-05, -8.3603e-05,\n",
      "          -1.5323e-04, -3.9436e-04, -9.4973e-05, -2.3567e-04,  1.4533e-04,\n",
      "           3.2532e-04,  1.8546e-04, -1.1472e-05,  5.9369e-05, -1.1783e-04,\n",
      "           8.4208e-06, -2.1603e-05, -1.3158e-04, -1.3676e-04, -1.5890e-05,\n",
      "           1.6665e-04,  4.0144e-05,  1.1149e-04,  4.7195e-05,  5.5209e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.6148e-05, -1.9772e-05,  9.8958e-05,  5.3043e-05, -4.8784e-05,\n",
      "          -1.2510e-04, -2.7664e-04, -1.4935e-04, -1.3164e-04,  5.2329e-05,\n",
      "          -6.4598e-05, -6.0001e-05, -3.0850e-04, -1.3617e-04, -1.3542e-04,\n",
      "           2.2626e-05, -9.1550e-05, -2.7678e-04, -2.0591e-04, -3.7310e-05,\n",
      "           1.3931e-04,  4.9951e-05,  1.4238e-04,  3.1974e-05,  1.7521e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.5520e-05,  2.4842e-06,  8.4902e-05,  6.8401e-05, -1.5350e-04,\n",
      "          -2.1434e-04, -4.1744e-04, -2.2331e-04,  9.4447e-06,  1.0276e-04,\n",
      "          -3.4464e-04, -4.6972e-04, -8.2325e-04,  4.6263e-05,  6.8535e-05,\n",
      "           2.8579e-04, -8.4629e-05, -2.4850e-04, -6.1892e-05,  2.7025e-04,\n",
      "           5.1961e-04,  2.2483e-04,  3.2212e-04,  4.9646e-05,  6.7047e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.6671e-05, -6.6907e-05,  4.5180e-05, -9.7921e-06, -2.2933e-04,\n",
      "          -1.6133e-04, -3.1830e-04,  8.5575e-05,  3.2668e-04,  5.6148e-04,\n",
      "           4.2871e-04,  8.9163e-05, -1.1517e-04,  7.5171e-05,  1.3926e-04,\n",
      "          -1.4302e-04, -4.8094e-04, -4.8875e-04, -4.1562e-04,  3.0640e-05,\n",
      "           3.3691e-04,  4.4003e-05,  8.7615e-05,  3.5958e-05,  8.4427e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.3133e-05, -8.3990e-05,  5.4928e-06, -5.2912e-05, -2.2489e-05,\n",
      "          -5.4018e-05,  1.9516e-05,  1.8535e-04,  6.4236e-04,  6.7799e-04,\n",
      "           4.4872e-04,  2.1766e-06, -3.0951e-04,  4.6689e-05, -1.9438e-04,\n",
      "          -1.7186e-04, -8.3955e-04, -4.3195e-04, -2.0531e-04,  8.9323e-05,\n",
      "           2.6849e-04,  1.0333e-04,  2.2161e-04,  1.3448e-04,  1.7523e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.4877e-05, -7.7527e-05,  2.8470e-05,  7.1220e-05,  3.4163e-05,\n",
      "           8.9683e-05,  5.2346e-05,  1.9580e-04,  4.1333e-04,  5.5215e-04,\n",
      "           2.5166e-04,  2.9949e-04, -1.7296e-04,  3.0301e-04,  1.7301e-04,\n",
      "           5.9547e-05, -3.4647e-04, -3.4348e-04, -2.7305e-04,  1.0751e-04,\n",
      "           2.8216e-04,  2.6885e-05,  1.7275e-04,  4.3587e-05,  1.0988e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.6406e-05,  1.2584e-05,  7.0752e-05,  9.7072e-05,  1.1249e-04,\n",
      "           1.0811e-04,  1.8371e-04,  1.4003e-04,  4.0009e-04,  3.0332e-04,\n",
      "           4.6818e-04, -5.4075e-05,  2.5336e-04,  1.7957e-04,  2.0502e-04,\n",
      "           8.1934e-05, -3.0925e-04, -1.4859e-04, -4.2887e-04,  2.0959e-04,\n",
      "           1.9633e-04,  1.8303e-04,  2.9696e-04,  5.4984e-05,  1.2915e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.6524e-05, -6.5538e-05, -1.1609e-04, -5.3783e-05, -1.6311e-04,\n",
      "          -5.3959e-05,  4.9872e-05,  8.8600e-05,  3.9008e-04,  3.8890e-04,\n",
      "           5.9440e-04,  4.4040e-04,  3.7480e-04,  6.2693e-04,  1.7390e-04,\n",
      "           2.1043e-04, -3.2691e-04, -1.9710e-04, -3.9512e-04, -2.6438e-04,\n",
      "          -1.6089e-04, -1.4422e-04,  1.0152e-04,  6.0303e-05,  1.1730e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.9034e-05,  3.6921e-05, -9.4081e-05, -2.5317e-05, -1.6251e-04,\n",
      "          -1.1752e-04, -7.3525e-05, -1.7878e-04,  3.8631e-05, -2.1387e-04,\n",
      "           1.0766e-04, -2.4115e-04,  4.2874e-04,  2.5761e-04,  2.6029e-04,\n",
      "          -2.2314e-05, -2.9783e-04, -3.1605e-04, -3.9395e-04, -3.4854e-04,\n",
      "          -2.9974e-04, -1.1920e-04, -3.6266e-05, -5.9268e-06,  4.4657e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6512e-05, -1.2363e-05, -3.5546e-05, -9.2908e-05, -1.1000e-04,\n",
      "          -1.5748e-04, -1.7809e-04, -1.9258e-04, -2.2908e-04,  6.2679e-05,\n",
      "           1.9761e-04,  4.5036e-04,  8.1348e-04,  7.0386e-04,  5.0009e-04,\n",
      "           1.3291e-04, -2.7144e-04, -2.9937e-04, -3.9342e-04, -1.9899e-04,\n",
      "          -1.6533e-04, -3.0968e-04, -4.1244e-05, -1.0680e-04, -3.0579e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.0320e-05,  5.3744e-05,  3.7538e-05,  6.4272e-06,  1.2721e-05,\n",
      "          -1.1444e-04, -1.7465e-04, -1.6117e-04, -4.9672e-04, -9.3487e-05,\n",
      "          -1.5215e-04,  1.8250e-04,  8.5715e-04,  3.5525e-04,  5.2909e-04,\n",
      "          -1.9252e-04,  2.0011e-05, -7.5292e-05,  8.1691e-05, -1.1422e-04,\n",
      "          -2.6628e-04, -2.3321e-04, -7.7584e-05, -1.9898e-04, -2.4504e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.7622e-05, -7.1969e-05, -4.3461e-05, -6.4441e-05, -1.2733e-04,\n",
      "          -1.2171e-04, -1.3006e-04, -1.5988e-04, -2.2876e-04, -8.9093e-05,\n",
      "           1.6602e-05,  1.5538e-04,  4.9678e-04,  2.8761e-04,  4.6258e-06,\n",
      "          -2.8942e-05,  1.3858e-04,  1.1854e-04,  5.2258e-05, -1.3882e-04,\n",
      "          -9.6744e-05, -1.2269e-04,  1.4312e-04, -5.9243e-05,  1.0970e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.7277e-05, -3.7297e-05,  4.7538e-05, -4.0606e-05, -3.3942e-05,\n",
      "          -1.0100e-04, -3.1698e-04, -2.8111e-04, -5.3201e-04, -1.2610e-04,\n",
      "          -2.1733e-04, -4.4719e-05,  2.8888e-05, -1.0762e-04, -1.4917e-04,\n",
      "          -9.7426e-05,  8.4192e-05,  4.6928e-05,  7.0049e-05,  1.7834e-04,\n",
      "           1.9152e-04,  2.1019e-04,  3.0600e-04, -5.4866e-05, -2.4583e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.4740e-05, -7.3835e-05, -6.8769e-05, -5.5331e-05, -4.7962e-05,\n",
      "          -6.3604e-05, -2.3241e-04, -2.2213e-04, -4.2905e-04, -3.1821e-04,\n",
      "          -2.3613e-04, -1.0747e-04, -5.1546e-05, -7.2706e-05, -6.0013e-05,\n",
      "           4.9090e-06,  1.3138e-04, -2.8963e-05,  2.1536e-05, -6.7151e-05,\n",
      "           4.4369e-05, -2.7294e-05, -3.6076e-05, -9.2556e-05, -1.0845e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.2641e-05,  3.2436e-06, -4.6383e-05,  1.8655e-06, -1.2543e-05,\n",
      "           1.5668e-05, -1.3502e-04, -1.6352e-04, -2.8805e-04, -1.8159e-04,\n",
      "          -7.4518e-05, -9.7784e-05, -1.2776e-04, -8.7879e-05, -2.4128e-04,\n",
      "          -1.6159e-04, -3.3799e-04, -2.2345e-05,  2.5917e-05,  3.3788e-05,\n",
      "          -3.7144e-05, -6.4908e-05, -2.5516e-04, -2.1168e-04, -1.7790e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.5371e-05, -7.0934e-05, -1.0958e-04, -9.9958e-05, -7.5238e-05,\n",
      "          -6.4487e-05, -1.5552e-04, -3.7787e-05, -1.8777e-04, -4.3828e-05,\n",
      "           9.0769e-05,  2.7426e-05,  1.1466e-04,  1.9196e-04,  6.8120e-05,\n",
      "           1.0881e-04, -9.7995e-05,  2.8896e-05, -8.3171e-05, -1.4789e-04,\n",
      "          -1.0666e-04, -1.3621e-04, -2.2117e-04, -1.1621e-04, -1.2460e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.6350e-05, -1.5665e-05, -3.8526e-05, -3.0436e-05,  3.3128e-05,\n",
      "           5.3533e-05,  9.1231e-05,  2.4331e-05,  1.1891e-04, -2.4767e-05,\n",
      "           1.4925e-04,  9.1994e-05,  2.9005e-04,  2.4739e-04,  1.9473e-04,\n",
      "           9.7650e-05,  8.1571e-05,  1.0657e-04, -7.9890e-05, -1.0911e-04,\n",
      "          -2.7207e-04, -2.3813e-05, -2.4624e-04, -7.7366e-05, -1.2884e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5349e-05, -1.8843e-05,  1.0170e-05, -2.2436e-05,  3.8216e-05,\n",
      "          -2.6206e-05, -5.6628e-05, -2.6622e-05, -1.1964e-04, -5.3741e-05,\n",
      "          -5.5133e-05,  5.8806e-05,  2.4563e-04,  2.6277e-04,  2.4339e-04,\n",
      "           1.6634e-04,  2.2939e-04,  8.8167e-05,  1.6990e-05, -1.0874e-04,\n",
      "          -9.1084e-05, -3.5359e-05, -8.5218e-05, -7.4645e-05, -1.3134e-04,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2586e-05,  5.0318e-05,  9.3789e-05,  9.5146e-05,  1.2613e-04,\n",
      "           5.2470e-05,  2.0219e-05,  4.2714e-05, -1.3838e-04, -3.6939e-05,\n",
      "          -1.4029e-04, -5.6361e-05,  2.2047e-05,  5.4905e-05,  2.2275e-04,\n",
      "          -7.6070e-06,  3.0288e-04,  1.2595e-04,  1.6205e-04,  1.0950e-05,\n",
      "          -5.4299e-05, -2.7670e-05, -9.9177e-05, -1.0842e-04, -5.9403e-05,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../networks')\n",
    "from starter import CNN\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN().to(device)\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "checkpoint = torch.load(\"../networks/trained_networks/trained_starter\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.train()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader( \n",
    "    datasets.MNIST('../networks/data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                   ])),\n",
    "        batch_size=1, shuffle=True)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    visualize_helper_selftrained(model, tensor=data, k=target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
