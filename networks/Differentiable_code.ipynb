{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sept 20 16:43:08 2018\n",
    "For: Starter_kit_cnns_pytorch\n",
    "Author: Gaurav_Shrivastava \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy.stats import norm \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../saliency')\n",
    "from network_saliency import visualize_helper_selftrained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution Neural Network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 784)\n",
    "        self.fc3 = nn.Linear(784, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x#F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training of model\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        noise_sd = torch.randn_like(data, device=device) * 0.25\n",
    "        data = data + noise_sd\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    # Training settings\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# for inputs,target in train_loader:\n",
    "#     break\n",
    "# # inputs = enumerate(next(train_loader))\n",
    "# print(torch.randn_like(inputs).shape)#* 2*torch.ones(1,28,28) )\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "#     test(model, device, test_loader)\n",
    "\n",
    "# torch.save(model,'trained_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_argmax(x):\n",
    "    beta = 2\n",
    "    # x = torch.Tensor(np.array([[.2, .0, .81, .53, .8]]))\n",
    "    a = torch.exp(beta*x)\n",
    "    b = torch.sum(torch.exp(beta*x))\n",
    "#     print(a,b)\n",
    "    softmax = a/b\n",
    "    max = torch.sum(softmax*x,1)\n",
    "#     print(max)\n",
    "    pos = x.size()\n",
    "    \n",
    "    softargmax = torch.sum(softmax*torch.arange(0,pos[1]))\n",
    "    return softargmax\n",
    "#     print(pos, softargmax)\n",
    "#     mx = softargmax.int()\n",
    "#     ans = softargmax.round()\n",
    "# #     print(mx)\n",
    "#     if ans>mx:\n",
    "#         return mx +1.0\n",
    "#     return mx + 0.0\n",
    "\n",
    "#     print(softargmax.int())#,softmax*torch.arange(0,pos[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smooth(object):\n",
    "#     \"\"\"A smoothed classifier g \"\"\"\n",
    "#     def __init__(self, base_classifier: torch.nn.Module, sigma, epsilon = 0.2):\n",
    "#         \"\"\"\n",
    "#         :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "#         :param num_classes:\n",
    "#         :param sigma: the noise level hyperparameter\n",
    "#         :param epsilon: hyperparameter for level of error\n",
    "#         \"\"\"\n",
    "#         self.base_classifier = base_classifier\n",
    "#         self.sigma = sigma.view(1,28,28)\n",
    "#         self.target = None\n",
    "#         self.epsilon = epsilon\n",
    "        \n",
    "    \"\"\"A smoothed classifier g \"\"\"\n",
    "    def __init__(self, base_classifier: torch.nn.Module, epsilon = 0.2):\n",
    "        \"\"\"\n",
    "        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "        :param num_classes:\n",
    "        :param sigma: the noise level hyperparameter\n",
    "        :param epsilon: hyperparameter for level of error\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.sigma = None\n",
    "        self.target = None\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def certify(self, x: torch.tensor, sigma, target, n: int, batch_size: int):\n",
    "        \"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n",
    "        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n",
    "        robust within a L2 ball of radius R around x.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n: the number of Monte Carlo samples to use for estimation\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: (predicted class, certified radius)\n",
    "        \"\"\"\n",
    "        self.sigma = sigma.view(1,28,28)\n",
    "        self.target = target +0.0\n",
    "#         print(target.dtype)\n",
    "        self.base_classifier.eval()\n",
    "        # draw samples of f(x+ epsilon)\n",
    "        cAHat, pABar = self.sample_noise(x,n,batch_size)\n",
    "#         print(self.sigma)\n",
    "        if pABar > 0.95:\n",
    "            pABar = 0.95\n",
    "        if pABar <0.5:\n",
    "            radius = self.sigma* 0.0\n",
    "        else:\n",
    "            radius = self.sigma * norm.ppf(pABar)\n",
    "#         print(pABar,norm.ppf(pABar))#,radius)\n",
    "        return cAHat, radius\n",
    "\n",
    "\n",
    "    def sample_noise(self, x: torch.tensor, num: int, batch_size) -> np.ndarray:\n",
    "        \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n",
    "\n",
    "        :param x: the input [channel x width x height]\n",
    "        :param num: number of samples to collect\n",
    "        :param batch_size:\n",
    "        :return: an ndarray[int] of length num_classes containing the per-class counts\n",
    "        \"\"\"\n",
    "        x = x.view(1,28,28)\n",
    "#         with torch.no_grad():\n",
    "        counts = 0\n",
    "        for _ in range(ceil(num / batch_size)):\n",
    "            this_batch_size = min(batch_size, num)\n",
    "            num -= this_batch_size\n",
    "            batch = x.repeat((this_batch_size, 1, 1, 1))\n",
    "            noise = torch.randn_like(batch, device=device) * self.sigma\n",
    "            scores = self.base_classifier(batch + noise).detach()#.argmax(1)\n",
    "            predictions = []\n",
    "            for i in range(len(scores)):\n",
    "                arg_score =  soft_argmax(scores[i].view(1,-1))\n",
    "                if torch.abs(arg_score - self.target)<self.epsilon:\n",
    "                    counts +=1\n",
    "                predictions.append(arg_score)\n",
    "#             counts += self._count_arr(predictions, self.num_classes)\n",
    "            return torch.stack(predictions).mean(), counts/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training of model\n",
    "def train_sigma_network(sigma_model, certified_model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    sigma_model.train()\n",
    "    for batch_idx, ((data, gradient), target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target + 0.0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sigma = torch.zeros([len(data),28,28])\n",
    "        \n",
    "        # Find the saliency map and pass that into the network\n",
    "#         for i in range(len(data)):\n",
    "#             temp_data = data[i][np.newaxis, ...]\n",
    "#             gradients, max_gradients = visualize_helper_selftrained(model, tensor=temp_data, k=target[i])\n",
    "#             max_gradients = max_gradients[np.newaxis, ...]\n",
    "        sigma = torch.abs(sigma_model(gradient).view(sigma.shape))\n",
    "        \n",
    "        total_loss = 0\n",
    "        pred_loss = 0\n",
    "        R_loss = 0\n",
    "        for i in range(len(data)):\n",
    "            pred_score, R = certified_model.certify(data[i],sigma[i],target[i],100,1)\n",
    "#             print(R.max(),R.min(), pred_score - target[i])\n",
    "            pred_loss += torch.abs(pred_score - target[i])#F.mse_loss(pred_score, target[i].int())\n",
    "            R_loss -= abs(R).mean()\n",
    "#         print(R_loss,pred_loss)\n",
    "        total_loss = pred_loss + R_loss\n",
    "#         print()\n",
    "        clip_value = 0.1\n",
    "#         for p in sigma_model.parameters():\n",
    "#             p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss.item()))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_model = FCN().to(device)\n",
    "optimizer = optim.SGD(sigma_model.parameters(), lr=0.0001, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, IO, List, Optional, Tuple, Union\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class MNIST_Extension(datasets.MNIST):\n",
    "    resources = [\n",
    "        (\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "    ) -> None:\n",
    "        super(MNIST_Extension, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform, download = download)\n",
    "        \n",
    "    def _check_exists(self) -> bool:\n",
    "        return (os.path.exists(os.path.join(self.processed_folder,\n",
    "                                            self.training_file)) and\n",
    "                os.path.exists(os.path.join(self.processed_folder,\n",
    "                                            self.test_file)))\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        elem = img.to(device)\n",
    "        temp_data = elem[np.newaxis, ...]\n",
    "        gradients, max_gradients = visualize_helper_selftrained(model, tensor=temp_data, k=val)\n",
    "        max_gradients = max_gradients[np.newaxis, ...]\n",
    "        new_data = ((elem, max_gradients), val)\n",
    "\n",
    "        return (img, max_gradients), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.078994\n",
      "Train Epoch: 10 [10/60000 (0%)]\tLoss: -0.032768\n",
      "Train Epoch: 10 [20/60000 (0%)]\tLoss: -0.032264\n",
      "Train Epoch: 10 [30/60000 (0%)]\tLoss: -0.032576\n",
      "Train Epoch: 10 [40/60000 (0%)]\tLoss: 0.999034\n",
      "Train Epoch: 10 [50/60000 (0%)]\tLoss: -0.032370\n",
      "Train Epoch: 10 [60/60000 (0%)]\tLoss: -0.033879\n",
      "Train Epoch: 10 [70/60000 (0%)]\tLoss: -0.032443\n",
      "Train Epoch: 10 [80/60000 (0%)]\tLoss: -0.019271\n",
      "Train Epoch: 10 [90/60000 (0%)]\tLoss: -0.031965\n",
      "Train Epoch: 10 [100/60000 (0%)]\tLoss: 0.043375\n",
      "Train Epoch: 10 [110/60000 (0%)]\tLoss: -0.031879\n",
      "Train Epoch: 10 [120/60000 (0%)]\tLoss: -0.015360\n",
      "Train Epoch: 10 [130/60000 (0%)]\tLoss: -0.032327\n",
      "Train Epoch: 10 [140/60000 (0%)]\tLoss: -0.033574\n",
      "Train Epoch: 10 [150/60000 (0%)]\tLoss: -0.032392\n",
      "Train Epoch: 10 [160/60000 (0%)]\tLoss: 5.969018\n",
      "Train Epoch: 10 [170/60000 (0%)]\tLoss: -0.031835\n",
      "Train Epoch: 10 [180/60000 (0%)]\tLoss: -0.032253\n",
      "Train Epoch: 10 [190/60000 (0%)]\tLoss: 0.001940\n",
      "Train Epoch: 10 [200/60000 (0%)]\tLoss: -0.003070\n",
      "Train Epoch: 10 [210/60000 (0%)]\tLoss: 1.962799\n",
      "Train Epoch: 10 [220/60000 (0%)]\tLoss: 4.998421\n",
      "Train Epoch: 10 [230/60000 (0%)]\tLoss: -0.032343\n",
      "Train Epoch: 10 [240/60000 (0%)]\tLoss: -0.032652\n",
      "Train Epoch: 10 [250/60000 (0%)]\tLoss: -0.033509\n",
      "Train Epoch: 10 [260/60000 (0%)]\tLoss: -0.032991\n",
      "Train Epoch: 10 [270/60000 (0%)]\tLoss: -0.032426\n",
      "Train Epoch: 10 [280/60000 (0%)]\tLoss: -0.031779\n",
      "Train Epoch: 10 [290/60000 (0%)]\tLoss: -0.032657\n",
      "Train Epoch: 10 [300/60000 (0%)]\tLoss: -0.033275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-83f27d85c25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     batch_size=1, shuffle=True, **kwargs)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain_sigma_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcertified_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-8af5996b314d>\u001b[0m in \u001b[0;36mtrain_sigma_network\u001b[0;34m(sigma_model, certified_model, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         for p in sigma_model.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#             p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "checkpoint = torch.load(\"trained_networks/trained_starter\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.eval()\n",
    "\n",
    "sigma_model = FCN().to(device)\n",
    "optimizer = optim.SGD(sigma_model.parameters(), lr=0.0000001, momentum=momentum)\n",
    "# print(torch.abs(sigma_model(inputs)).view(inputs.shape)[0].shape)\n",
    "\n",
    "l1loss = nn.L1Loss()\n",
    "certified_model = Smooth(model) #,torch.abs(sigma_model(inputs)).view(inputs.shape)[0])\n",
    "\n",
    "\n",
    "# data = MNIST_Extension('./data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "\n",
    "# i = 0\n",
    "# for (elem, gradient), val in data:\n",
    "#     print(gradient)\n",
    "    \n",
    "#     sigma[i] = torch.abs(sigma_model(max_gradients).view(data[i].shape))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_Extension('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=True, **kwargs)\n",
    "\n",
    "train_sigma_network(sigma_model,certified_model,device,train_loader,optimizer,epoch,log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "sigma = sigma_model(inputs).view(inputs.shape)\n",
    "# print(sigma)\n",
    "torchvision.utils.save_image(sigma,'visual_sigma.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
