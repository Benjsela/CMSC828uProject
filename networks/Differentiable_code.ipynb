{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sept 20 16:43:08 2018\n",
    "For: Starter_kit_cnns_pytorch\n",
    "Author: Gaurav_Shrivastava \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy.stats import norm \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../saliency')\n",
    "from network_saliency import visualize_helper_selftrained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution Neural Network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 784)\n",
    "        self.fc3 = nn.Linear(784, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x#F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training of model\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        noise_sd = torch.randn_like(data, device=device) * 0.25\n",
    "        data = data + noise_sd\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    # Training settings\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# for inputs,target in train_loader:\n",
    "#     break\n",
    "# # inputs = enumerate(next(train_loader))\n",
    "# print(torch.randn_like(inputs).shape)#* 2*torch.ones(1,28,28) )\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "#     test(model, device, test_loader)\n",
    "\n",
    "# torch.save(model,'trained_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_argmax(x):\n",
    "    beta = 2\n",
    "    # x = torch.Tensor(np.array([[.2, .0, .81, .53, .8]]))\n",
    "    a = torch.exp(beta*x)\n",
    "    b = torch.sum(torch.exp(beta*x))\n",
    "#     print(a,b)\n",
    "    softmax = a/b\n",
    "    max = torch.sum(softmax*x,1)\n",
    "#     print(max)\n",
    "    pos = x.size()\n",
    "    \n",
    "    softargmax = torch.sum(softmax*torch.arange(0,pos[1]))\n",
    "    return softargmax\n",
    "#     print(pos, softargmax)\n",
    "#     mx = softargmax.int()\n",
    "#     ans = softargmax.round()\n",
    "# #     print(mx)\n",
    "#     if ans>mx:\n",
    "#         return mx +1.0\n",
    "#     return mx + 0.0\n",
    "\n",
    "#     print(softargmax.int())#,softmax*torch.arange(0,pos[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smooth(object):\n",
    "#     \"\"\"A smoothed classifier g \"\"\"\n",
    "#     def __init__(self, base_classifier: torch.nn.Module, sigma, epsilon = 0.2):\n",
    "#         \"\"\"\n",
    "#         :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "#         :param num_classes:\n",
    "#         :param sigma: the noise level hyperparameter\n",
    "#         :param epsilon: hyperparameter for level of error\n",
    "#         \"\"\"\n",
    "#         self.base_classifier = base_classifier\n",
    "#         self.sigma = sigma.view(1,28,28)\n",
    "#         self.target = None\n",
    "#         self.epsilon = epsilon\n",
    "        \n",
    "    \"\"\"A smoothed classifier g \"\"\"\n",
    "    def __init__(self, base_classifier: torch.nn.Module, epsilon = 0.2):\n",
    "        \"\"\"\n",
    "        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "        :param num_classes:\n",
    "        :param sigma: the noise level hyperparameter\n",
    "        :param epsilon: hyperparameter for level of error\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.sigma = None\n",
    "        self.target = None\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def certify(self, x: torch.tensor, sigma, target, n: int, batch_size: int):\n",
    "        \"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n",
    "        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n",
    "        robust within a L2 ball of radius R around x.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n: the number of Monte Carlo samples to use for estimation\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: (predicted class, certified radius)\n",
    "        \"\"\"\n",
    "        self.sigma = sigma.view(1,28,28)\n",
    "        self.target = target +0.0\n",
    "#         print(target.dtype)\n",
    "        self.base_classifier.eval()\n",
    "        # draw samples of f(x+ epsilon)\n",
    "        cAHat, pABar = self.sample_noise(x,n,batch_size)\n",
    "#         print(self.sigma)\n",
    "        if pABar > 0.95:\n",
    "            pABar = 0.95\n",
    "        if pABar <0.5:\n",
    "            radius = self.sigma* 0.0\n",
    "        else:\n",
    "            radius = self.sigma * norm.ppf(pABar)\n",
    "#         print(pABar,norm.ppf(pABar))#,radius)\n",
    "        return cAHat, radius\n",
    "\n",
    "\n",
    "    def sample_noise(self, x: torch.tensor, num: int, batch_size) -> np.ndarray:\n",
    "        \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n",
    "\n",
    "        :param x: the input [channel x width x height]\n",
    "        :param num: number of samples to collect\n",
    "        :param batch_size:\n",
    "        :return: an ndarray[int] of length num_classes containing the per-class counts\n",
    "        \"\"\"\n",
    "        x = x.view(1,28,28)\n",
    "#         with torch.no_grad():\n",
    "        counts = 0\n",
    "        for _ in range(ceil(num / batch_size)):\n",
    "            this_batch_size = min(batch_size, num)\n",
    "            num -= this_batch_size\n",
    "            batch = x.repeat((this_batch_size, 1, 1, 1))\n",
    "            noise = torch.randn_like(batch, device=device) * self.sigma\n",
    "            scores = self.base_classifier(batch + noise).detach()#.argmax(1)\n",
    "            predictions = []\n",
    "            for i in range(len(scores)):\n",
    "                arg_score =  soft_argmax(scores[i].view(1,-1))\n",
    "                if torch.abs(arg_score - self.target)<self.epsilon:\n",
    "                    counts +=1\n",
    "                predictions.append(arg_score)\n",
    "#             counts += self._count_arr(predictions, self.num_classes)\n",
    "            return torch.stack(predictions).mean(), counts/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training of model\n",
    "def train_sigma_network(sigma_model, certified_model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    sigma_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target + 0.0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sigma = torch.zeros([len(data),28,28])\n",
    "        \n",
    "        # Find the saliency map and pass that into the network\n",
    "        for i in range(len(data)):\n",
    "            temp_data = data[i][np.newaxis, ...]\n",
    "            gradients, max_gradients = visualize_helper_selftrained(model, tensor=temp_data, k=target[i])\n",
    "            max_gradients = max_gradients[np.newaxis, ...]\n",
    "            sigma[i] = torch.abs(sigma_model(max_gradients).view(data[i].shape))\n",
    "        \n",
    "        total_loss = 0\n",
    "        pred_loss = 0\n",
    "        R_loss = 0\n",
    "        for i in range(len(data)):\n",
    "            pred_score, R = certified_model.certify(data[i],sigma[i],target[i],100,1)\n",
    "#             print(R.max(),R.min(), pred_score - target[i])\n",
    "            pred_loss += torch.abs(pred_score - target[i])#F.mse_loss(pred_score, target[i].int())\n",
    "            R_loss -= abs(R).mean()\n",
    "#         print(R_loss,pred_loss)\n",
    "        total_loss = pred_loss + R_loss\n",
    "#         print()\n",
    "        clip_value = 0.1\n",
    "#         for p in sigma_model.parameters():\n",
    "#             p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss.item()))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_model = FCN().to(device)\n",
    "optimizer = optim.SGD(sigma_model.parameters(), lr=0.0001, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 47.705006\n",
      "Train Epoch: 10 [1000/60000 (2%)]\tLoss: 26.090038\n",
      "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 35.840435\n",
      "Train Epoch: 10 [3000/60000 (5%)]\tLoss: 45.792679\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 44.619518\n",
      "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 47.875221\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 63.767998\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "lr = .01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "checkpoint = torch.load(\"trained_networks/trained_starter\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.eval()\n",
    "\n",
    "sigma_model = FCN().to(device)\n",
    "optimizer = optim.SGD(sigma_model.parameters(), lr=0.0000001, momentum=momentum)\n",
    "# print(torch.abs(sigma_model(inputs)).view(inputs.shape)[0].shape)\n",
    "\n",
    "l1loss = nn.L1Loss()\n",
    "certified_model = Smooth(model) #,torch.abs(sigma_model(inputs)).view(inputs.shape)[0])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=100, shuffle=True, **kwargs)\n",
    "\n",
    "train_sigma_network(sigma_model,certified_model,device,train_loader,optimizer,epoch,log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "sigma = sigma_model(inputs).view(inputs.shape)\n",
    "# print(sigma)\n",
    "torchvision.utils.save_image(sigma,'visual_sigma.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
