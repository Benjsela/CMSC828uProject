{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if on a Mac\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pulled from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulled from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Because we are just using MNIST we'll rely on Alex Net\n",
    "model = models.alexnet(pretrained=True)\n",
    "set_parameter_requires_grad(model, True)\n",
    "input_size = 224\n",
    "\n",
    "# 10 for the 10 digits\n",
    "num_classes = 10\n",
    "\n",
    "# Update final fcl of the network (you'll need to change this if we don't use Alex Net)\n",
    "model.classifier[6] = nn.Linear(4096,num_classes)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "mnist_trainset = MNIST(root='./data', train=True, download=True, transform=data_transforms['train'])\n",
    "mnist_valset = MNIST(root='./data', train=True, download=True, transform=data_transforms['val'])\n",
    "image_datasets = {'train': mnist_trainset, 'val' : mnist_valset}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We're just doing feature extraction so we don't want to optimize much\n",
    "params_to_update = model.parameters()\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "optimizer = optim.Adam(params_to_update, lr = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.2424 Acc: 0.5551\n",
      "val Loss: 0.9520 Acc: 0.8078\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2.4816 Acc: 0.5765\n",
      "val Loss: 0.7620 Acc: 0.8562\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 2.6577 Acc: 0.5772\n",
      "val Loss: 0.6842 Acc: 0.8706\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 2.6250 Acc: 0.5839\n",
      "val Loss: 0.9424 Acc: 0.8262\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 2.5889 Acc: 0.5879\n",
      "val Loss: 1.2628 Acc: 0.8134\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 2.5673 Acc: 0.5899\n",
      "val Loss: 0.5224 Acc: 0.8950\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 2.6774 Acc: 0.5857\n",
      "val Loss: 0.9435 Acc: 0.8312\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 2.6413 Acc: 0.5890\n",
      "val Loss: 0.6823 Acc: 0.8672\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 2.7106 Acc: 0.5849\n",
      "val Loss: 0.8625 Acc: 0.8440\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 2.6958 Acc: 0.5893\n",
      "val Loss: 0.6532 Acc: 0.8701\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 2.7055 Acc: 0.5857\n",
      "val Loss: 1.3466 Acc: 0.7996\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 2.7187 Acc: 0.5900\n",
      "val Loss: 1.2979 Acc: 0.8057\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 2.6759 Acc: 0.5873\n",
      "val Loss: 0.9901 Acc: 0.8330\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 2.7193 Acc: 0.5853\n",
      "val Loss: 0.6578 Acc: 0.8783\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 2.6321 Acc: 0.5910\n",
      "val Loss: 0.6133 Acc: 0.8802\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 2.6422 Acc: 0.5887\n",
      "val Loss: 0.7318 Acc: 0.8616\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 2.7127 Acc: 0.5863\n",
      "val Loss: 0.8823 Acc: 0.8460\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 2.7002 Acc: 0.5864\n",
      "val Loss: 0.8227 Acc: 0.8400\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 2.6751 Acc: 0.5878\n",
      "val Loss: 0.6666 Acc: 0.8704\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 2.7013 Acc: 0.5837\n",
      "val Loss: 1.0077 Acc: 0.8320\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 2.6732 Acc: 0.5888\n",
      "val Loss: 0.8593 Acc: 0.8456\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 2.7683 Acc: 0.5841\n",
      "val Loss: 1.0374 Acc: 0.8351\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 2.6928 Acc: 0.5937\n",
      "val Loss: 0.6109 Acc: 0.8711\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 2.6812 Acc: 0.5882\n",
      "val Loss: 0.6978 Acc: 0.8691\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 2.6972 Acc: 0.5903\n",
      "val Loss: 1.4430 Acc: 0.7773\n",
      "\n",
      "Training complete in 3370m 38s\n",
      "Best val Acc: 0.895033\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
